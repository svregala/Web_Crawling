# Web_Crawling
 
For the full, detailed project description, please take a look at Project_Description.pdf. To run the project, all the necessary files are under MyCrawlerProject/src, which are: Controller.java, MyCrawler.java, CrawlResult.java, Discovered.java, Fetched.java, and Visited.java. Note: the project optimally runs in Eclipse coding environment with the corresponding external jar files (under jar_files -- add these to the project dependencies). Click "Run Project" to perform web crawl.

**Synopsis**: In this project, we expand on an existing web crawler to measure the aspects of a crawl, study the characteristics of the crawl, download web pages from the crawl, and gather webpage metadata. We can choose which news sites to crawl by editing some of the code, but for this repository, we crawl the USA Today news site (https://www.usatoday.com). After running, the crawl creates 3 CSV files (fetch_usatoday.csv, urls_usatoday.csv, visit_usatoday,csv) and 1 text file (CrawlReport_usatoday.txt):
- urls_usatoday.csv -- includes all of the URLs (including repeats) that were discovered and processed in some way (2 column spreadsheet where column 1 contained the encountered URL & column 2 an indicator of whether the URL resides in the website or points outside of the website, represented as OK and N_OK respectively) -- not included in the repository due to inherently large size
- fetch_usatoday.csv -- includes the URLs it attempts to fetch (2 column spreadsheet where column 1 contains the URL and column 2 contains HTTP/HTTPS status code received)
- visit_usatoday.csv -- includes all the files it successfully downloads (4 column spreadsheet where column 1 contains the URLs successfully downloaded, column 2 contains the size of the downloaded file in bytes, column 3 contains the nyumber of outlinks found, and column 4 conatins the resulting content-type (e.g. text/html, image/jpeg, image/png, etc.))
- CrawlReport_usatoday.txt -- includes statistics and data such as fetch statistics (number of fetches attempted/succeeded/failed/aborted), outgoing URLs (total URLs, number of unique URLs extracted/within/outside USA Today site), status codes (200, 301, 401, etc.), file sizes, and content types (text/html, image/gif, image/png, etc.).